get_url <- function(year, quarter){
url = paste0(
"https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/",
year, "-", quarter,
".csv"
)
return(url)
}
load_file_get_fname <- function(year, quarter){
url = get_url(year, quarter)
result = tryCatch(
{
print(paste0("Downloading for ", year, " ", quarter))
read.csv(url)
},
error = function(e) {
print(paste0("The url might not exist, no data for ", year, quarter, " is loaded."))
NULL}
)
fname = paste0(year, "-", quarter, ".csv")
path = paste0("./data/", fname)
if(length(result)!=0){
write.csv(result, path, row.names = FALSE)
} else {
fname = NULL
}
return(fname)
}
lst_fnames = character()
four_quarters = paste("Q", 1:4, sep = "")
for(year in 2009:2019){
for(quarter in four_quarters){
i = length(lst_fnames) + 1
fname = load_file_get_fname(year, quarter)
if(length(fname)!=0){
lst_fnames[i] = fname
}
}
}
write(lst_fnames, "data/lst_fnames.txt")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/all_dfs.RDS"
download.file(path,"./bail_out/all_dfs.RDS", method="curl")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/df_tot.RDSS"
download.file(path,"./bail_out/df_tot.RDS", method="curl")
get_url <- function(year, quarter){
url = paste0(
"https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/",
year, "-", quarter,
".csv"
)
return(url)
}
load_file_get_fname <- function(year, quarter){
url = get_url(year, quarter)
result = tryCatch(
{
print(paste0("Downloading for ", year, " ", quarter))
read.csv(url)
},
error = function(e) {
print(paste0("The url might not exist, no data for ", year, quarter, " is loaded."))
NULL}
)
fname = paste0(year, "-", quarter, ".csv")
path = paste0("./data/", fname)
if(length(result)!=0){
write.csv(result, path, row.names = FALSE)
} else {
fname = NULL
}
return(fname)
}
lst_fnames = character()
four_quarters = paste("Q", 1:4, sep = "")
for(year in 2009:2019){
for(quarter in four_quarters){
i = length(lst_fnames) + 1
fname = load_file_get_fname(year, quarter)
if(length(fname)!=0){
lst_fnames[i] = fname
}
}
}
write(lst_fnames, "data/lst_fnames.txt")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/all_dfs.RDS"
download.file(path,"./bail_out/all_dfs.RDS", method="curl")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/df_tot.RDSS"
download.file(path,"./bail_out/df_tot.RDS", method="curl")
"
Once the functions above have been compiled, you need to run 'download_disbursement_csvs()'from within the
R_midterm directory (or passing the 'output_dir' parameter with the path to the 'data' directory within
the R_midterm directory).
"
"
Once the functions above have been compiled, you need to run 'download_disbursement_csvs()'from within the
R_midterm directory (or passing the 'output_dir' parameter with the path to the 'data' directory within
the R_midterm directory).
"
download_single_disb_file = function(url, output_fp, check.exists = T){
if( check.exists ){
if( file.exists( output_fp ) ){
print( sprintf("File %s already exists and will not be re-downloaded.", output_fp) )
return( T )
} else {
tryCatch(
{
print(sprintf("Downloading for %s",output_fp))
download.file(url, output_fp)
return( T )
},
error = function(e) {
print(sprintf("The url might not exist, no data for %s is loaded.", output_fp))
F
}
)
}
}
}
download_disbursement_csvs = function( years = 2009:2019, output_dir = './data/' ){
# urls = paste0( "https://projects.propublica.org/congress/assets/staffers/",
#                rep( years, each = 4),
#                paste0("Q", 1:4, sep = ""),
#                "-house-disburse-detail.csv"
# )
urls = paste0( "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/",
rep( years, each = 4), "-",
paste0("Q", 1:4, sep = ""),
".csv"
)
output_dir = gsub(pattern = '[/]+', replacement = '/', paste0(output_dir, '/'))
output_fps = paste0( output_dir, rep( years, each = 4), "-", paste0("Q", 1:4, sep = ""), '.csv' )
for( idx in seq_along(urls) ){
if( download_single_disb_file( urls[idx], output_fps[idx] ) ){
print( sprintf( "File %s is loaded.", output_fps[idx] ) )
} else {
print( sprintf( "File %s is *not* loaded.", output_fps[idx] ) )
}
}
output_list_fp = paste0(output_dir, 'lst_files.txt')
write( list.files(output_dir, pattern = '.csv'), output_list_fp )
output_list_fp
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/all_dfs.RDS"
download.file(path,"./bail_out/all_dfs.RDS", method="curl")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/df_tot.RDSS"
download.file(path,"./bail_out/df_tot.RDS", method="curl")
}
View(download_disbursement_csvs)
View(download_single_disb_file)
function(url, output_fp, check.exists = T){
if( check.exists ){
if( file.exists( output_fp ) ){
print( sprintf("File %s already exists and will not be re-downloaded.", output_fp) )
return( T )
} else {
tryCatch(
{
print(sprintf("Downloading for %s",output_fp))
download.file(url, output_fp)
return( T )
},
error = function(e) {
print(sprintf("The url might not exist, no data for %s is loaded.", output_fp))
F
}
)
}
}
}
{
print(sprintf("Downloading for %s",output_fp))
download.file(url, output_fp)
return( T )
},
error = function(e) {
print(sprintf("The url might not exist, no data for %s is loaded.", output_fp))
F
}
View(download_disbursement_csvs)
function( years = 2009:2019, output_dir = './data/' ){
# urls = paste0( "https://projects.propublica.org/congress/assets/staffers/",
#                rep( years, each = 4),
#                paste0("Q", 1:4, sep = ""),
#                "-house-disburse-detail.csv"
# )
urls = paste0( "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/",
rep( years, each = 4), "-",
paste0("Q", 1:4, sep = ""),
".csv"
)
output_dir = gsub(pattern = '[/]+', replacement = '/', paste0(output_dir, '/'))
output_fps = paste0( output_dir, rep( years, each = 4), "-", paste0("Q", 1:4, sep = ""), '.csv' )
for( idx in seq_along(urls) ){
if( download_single_disb_file( urls[idx], output_fps[idx] ) ){
print( sprintf( "File %s is loaded.", output_fps[idx] ) )
} else {
print( sprintf( "File %s is *not* loaded.", output_fps[idx] ) )
}
}
output_list_fp = paste0(output_dir, 'lst_files.txt')
write( list.files(output_dir, pattern = '.csv'), output_list_fp )
output_list_fp
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/all_dfs.RDS"
download.file(path,"./bail_out/all_dfs.RDS", method="curl")
path = "https://nycdsastudent.s3.us-east-2.amazonaws.com/MidtermRDA/df_tot.RDSS"
download.file(path,"./bail_out/df_tot.RDS", method="curl")
}
View(download_disbursement_csvs)
library(tidyverse)
string2 <- 'If I want to include a "quote" inside a string, I use single quotes'
string1 <- "This is a string"
#> \
str_view(x, "an")
x
x <- c("apple", "banana", "pear")
str_view(x, "an")
?str_trim
?str_trim(x)
str_trim(x)
str_trim(x)
x= c('Violations was the one',
'they closed',
'We are opened',
'No Violations')
x[4]='cited')
str_replace_all(x, c(x[1]='violations',
x[2]='closed',
x[3]='opened',
x[4]='cited'))
str_replace_all(x, c(x[1]='violations',
x[2]='closed',
x[3]='opened',
x[4]='no'))
str_replace_all(x, c('Violations was the one'='violations',
x[2]='closed',
x[3]='opened',
x[4]='no'))
x[1]
str_replace_all(x, c(x[1]='violations',
x[2]='closed',
x[3]='opened',
x[4]='no'))
str_replace_all(x, c('Violations was the one'='violations',
'they closed'='closed',
'We are opened'='opened',
'No Violations'='no'))
install.packages('plotly')
library(plotly)
shiny::runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks3 = starbucks2 %>%
filter(store_count > 0) %>%
group_by(country, store_count) %>%
summarise(num_store= n(store_count))
starbucks3 = starbucks2 %>%
filter(store_count > 0) %>%
group_by(country, store_count) %>%
summarise(num_store= sum(store_count))
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks3 = starbucks2 %>%
mutate(latlon= paste(latitude, longitude, sep = ":"))
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks3 = starbucks2 %>%
group_by(country) %>%
summarise(n())
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks3 = starbucks2 %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks3 = starbucks2 %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
starbucks3 = starbucks2 %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
gvisGeoChart(data=starbucks3,
locationvar = "country",
options=list(displayMode='regions'),
sizevar = 'num_store',
hovervar = 'country')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
unique(starbucks2)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
filter(country=='United States of America)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
starbucks2 %>%
filter(country=='United States of America')
starbucks2 = read.csv('./data/starbucks2.csv')
starbucks2 = read.csv('./data/starbucks2.csv')
starbucks2 = read.csv('./data/starbucks2.csv')
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
write.csv(starbucks2, './data/starbucks2.csv')
starbucks2 = read.csv('./data/starbucks2.csv')
starbucks2 = read.csv('./data/starbucks2.csv')
starbucks2 = read.csv('./data/starbucks2.csv', stringsAsFactors = F)
starbucks2 = read.csv('./data/starbucks2.csv', stringsAsFactors = F)
runApp('C:/Users/randy/NYCDSA_Projects/shiny')
starbucks2 = read.csv('./data/starbucks2.csv', stringsAsFactors = F)
starbucks2 = read.csv('./data/starbucks2.csv', stringsAsFactors = F)
getwd()
setwd("C:/Users/randy/NYCDSA_Projects/shiny")
starbucks2 = read.csv('./data/starbucks2.csv', stringsAsFactors = F)
starbucks3 = starbucks2 %>%
filter(store >0) %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
library(shiny)
library(shinydashboard)
library(googleVis)
library(dplyr)
library(tibble)
library(tidyverse)
library(DT)
library(plotly)
starbucks3 = starbucks2 %>%
filter(store >0) %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
starbucks3 = starbucks2 %>%
filter(store_count >0) %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
head(starbucks2, 5)
head(starbucks2, 5)
runApp()
runApp()
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
code = code %>%
mutate(Country_Name= ifelse(Country_Name=='Azerbaijan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Georgia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Kazakhstan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Russian Federation' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Armenia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Korea' , 'South Korea',Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='United States of America' , 'United States',Country_Name))
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
code = read.csv('./data/country_code.csv')
code= code%>%
mutate(Country_Name= word(Country_Name, 1, sep=','))
code = code %>%
mutate(Country_Name= ifelse(Country_Name=='Azerbaijan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Georgia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Kazakhstan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Russian Federation' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Armenia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Korea' , 'South Korea',Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='United States of America' , 'United States',Country_Name))
code = code %>%
filter(Country_Name != 'NULL')
code= rename(code, continent= 'Continent_Name',
Country= 'Two_Letter_Country_Code')
code = code %>%
filter(Country_Name != '')
head(code, 5)
unique(code$Country_Name)
#Merge code and starbucks so we can have a complete for the country
starbucks2= full_join(starbucks, code, by='Country')
starbucks = read.csv('./data/starbucks.csv', encoding="UTF-8", stringsAsFactors=FALSE)
head(starbucks, 5)
starbucks$store_count = rep(1, dim(starbucks)[1])
starbucks= starbucks%>%
select(c(-Postcode, -Phone.Number))
unique(starbucks$Country)
code = read.csv('./data/country_code.csv')
code= code%>%
mutate(Country_Name= word(Country_Name, 1, sep=','))
code = code %>%
mutate(Country_Name= ifelse(Country_Name=='Azerbaijan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Georgia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Kazakhstan' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Russian Federation' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Armenia' & Continent_Name=='Asia', '', Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='Korea' , 'South Korea',Country_Name)) %>%
mutate(Country_Name= ifelse(Country_Name=='United States of America' , 'United States',Country_Name))
code = code %>%
filter(Country_Name != 'NULL')
code= rename(code, continent= 'Continent_Name',
Country= 'Two_Letter_Country_Code')
code = code %>%
filter(Country_Name != '')
head(code, 5)
unique(code$Country_Name)
#Merge code and starbucks so we can have a complete for the country
starbucks2= full_join(starbucks, code, by='Country')
starbucks2= starbucks2 %>%
mutate(store_count= ifelse(is.na(store_count), 0, store_count))
names(starbucks2) = tolower(names(starbucks2))
starbucks2= rename(starbucks2, code= 'country',
country ='country_name')
head(starbucks2, 5)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
write.csv(starbucks2, './data/starbucks3.csv')
runApp()
starbucks4 = starbucks3 %>%
filter(store_count >0) %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
starbucks3 = read.csv('./data/starbucks3.csv', stringsAsFactors = F)
starbucks4 = starbucks3 %>%
filter(store_count >0) %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
runApp()
runApp()
runApp()
starbucks4 = starbucks3 %>%
group_by(country) %>%
summarise(num_store= sum(store_count))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
sum(starbucks3$store_count)
dim(starbucks4)
unique(starbucks3$city)
sum(unique(starbucks3$city))
count(unique(starbucks3$city))
dim(unique(starbucks3$city))
length(unique(starbucks3$city))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
